# 深度学习

**特点**

* 使用多层神经网络，**自动提取数据的多层次特征**
* **适合处理非结构化数据**，如图像、音频、文本等
* 模型复杂，通常视为“**黑盒**”，**解释性差**、**依赖大量计算资源**、训练时间长

**应用场景**：金融服务、媒体和娱乐、客户服务、医疗卫生、工业自动化、自动驾驶、航空航天和军事、NLP

## 神经网络基础

> 神经网络基本结构 = 输入层 + 中间层（全连接层） + 输出层
>
> 前向传播 = 数据从输入层往输出层输送的过程

人工神经网络（ANN）：机器学习模型，是**多层感知机（MLP）模拟神经元**。每层神经元会对输入数据进行非线性变化，各层之间的神经元是全连接的。

多层感知机中每个神经元的数学公式：``y = h(x) * (b + w1 * x1 + w2 * x2)``，其中h(x)是一个阶跃函数（也是**激活函数**，**作用就是在神经网络中引入非线性**）

* 激活函数：

```python
# 阶跃函数
def step_function1(x):
    return np.array(x > 0, dtype = int)

# sigmoid 函数:将输入值映射到(0, 1)，当x为0时y = 0.5。常用在输出层。在[-6, 6]之外时，输出变化很小，可能导致信息丢失
def sigmoid(x):
    # return 1 / 1 + math.pow(math.e, -x)
    return 1 / np.exp(-x)

# tanh 函数：双曲正切函数，映射区间(-1, 1)。关于原点对称。常用在隐藏层。[-3, 3]以外的变化很小
def tanh(x):
    # return (1 - math.pow(math.e, -2 * x)) / (1 + math.pow(math.e, -2 * x))
    return np.tanh(x)

# sigmoid和tanh函数容易在深度神经网络的隐藏层中进行梯度传递时会出现“梯度消失”

# relu 函数：f(x) = max(0, x)
# 修正线性单元，将小于0的输入转换为0，大于等于0的输入则保持不变。常用于隐藏层
def relu(x):
    return np.maximum(0, x)

# leaky relu：在负区间引入一个小斜率来解决“大量死亡神经元”问题
t = -0.0001
def leaky_relu(x):
    return np.maximum(x, t * x)

# softmax：将一个任意的师叔向量转化为一个概率分布，确保输出值的总和为1，是二分类sigmoid在多分类的推广
# 常用在多分类问题的输出层，表示类别的预测概率
def softmax(x):
    # TODO 2025.10.20
    pass
```

